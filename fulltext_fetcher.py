#!/usr/bin/env python3
"""
å…¨æ–‡æŠ“å–æ¨¡å— - è§£å†³RSSåªæœ‰æ‘˜è¦çš„é—®é¢˜
ä½¿ç”¨å¤šä¸ªå¤‡é€‰æ–¹æ¡ˆï¼Œç¡®ä¿æ‹¿åˆ°å®Œæ•´æ­£æ–‡
"""

import argparse
import os
import re
import sqlite3
import time
import requests
from bs4 import BeautifulSoup
from readability import Document
import trafilatura

DB_PATH = os.path.join(os.path.dirname(__file__), 'data', 'ai_rss.db')

def fetch_full_text(url, retry=2):
    """
    å¤šç­–ç•¥å…¨æ–‡æŠ“å–ï¼š
    1. trafilatura (æœ€å¹²å‡€ï¼Œä¸“é—¨æå–æ­£æ–‡)
    2. readability (å¤‡é€‰)
    3. beautifulsoup æš´åŠ›æå– (å…œåº•)
    """
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }
    
    # ç­–ç•¥1: trafilatura - ç²¾åº¦æœ€é«˜
    try:
        downloaded = trafilatura.fetch_url(url)
        if downloaded:
            text = trafilatura.extract(downloaded, include_comments=False, include_tables=False)
            if text and len(text) > 500:
                print(f"  âœ… trafilatura æˆåŠŸ: {len(text)} å­—ç¬¦")
                return text
    except Exception as e:
        print(f"  âš ï¸ trafilatura å¤±è´¥: {e}")
    
    # ç­–ç•¥2: readability - é€šç”¨æ€§å¥½
    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        doc = Document(response.text)
        text = doc.summary()
        # æ¸…ç†HTMLæ ‡ç­¾
        soup = BeautifulSoup(text, 'html.parser')
        text = soup.get_text()
        if len(text) > 500:
            print(f"  âœ… readability æˆåŠŸ: {len(text)} å­—ç¬¦")
            return text
    except Exception as e:
        print(f"  âš ï¸ readability å¤±è´¥: {e}")
    
    # ç­–ç•¥3: æš´åŠ›æå– - æ­»é©¬å½“æ´»é©¬åŒ»
    try:
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')
        # ç§»é™¤è„šæœ¬å’Œæ ·å¼
        for script in soup(["script", "style", "nav", "header", "footer", "aside"]):
            script.decompose()
        text = soup.get_text()
        lines = (line.strip() for line in text.splitlines())
        chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
        text = ' '.join(chunk for chunk in chunks if chunk)
        if len(text) > 500:
            print(f"  âœ… æš´åŠ›æå– æˆåŠŸ: {len(text)} å­—ç¬¦")
            return text[:10000]  # æˆªæ–­
    except Exception as e:
        print(f"  âš ï¸ æš´åŠ›æå– å¤±è´¥: {e}")
    
    return None

def update_articles_with_fulltext(limit=50, force=False, feed_name=None, days=None):
    """
    ä¸ºcontentä¸ºç©ºæˆ–å¤ªçŸ­çš„æ–‡ç« è¡¥å…¨å…¨æ–‡
    force=True: å¼ºåˆ¶é‡æ–°æŠ“å–
    feed_name: ä»…å¤„ç†æŒ‡å®šæºï¼ˆç²¾ç¡®åŒ¹é…ï¼‰
    days: ä»…å¤„ç†æœ€è¿‘Nå¤©å†…çš„æ–‡ç« 
    """
    conn = sqlite3.connect(DB_PATH)
    conn.row_factory = sqlite3.Row
    c = conn.cursor()

    where = ["article_link LIKE 'http%'"]
    params = []

    if not force:
        where.append("(content IS NULL OR length(content) < 200 OR fulltext_fetched = 0)")

    if feed_name:
        where.append("feed_name = ?")
        params.append(feed_name)

    if days is not None:
        where.append("published_date >= datetime('now', ?)")
        params.append(f"-{int(days)} days")

    sql = f'''
        SELECT id, article_title, article_link
        FROM articles
        WHERE {' AND '.join(where)}
        ORDER BY published_date DESC
        LIMIT ?
    '''
    params.append(limit)
    c.execute(sql, params)
    articles = c.fetchall()

    print(f"ğŸ“„ å‘ç° {len(articles)} ç¯‡æ–‡ç« éœ€è¦æŠ“å–å…¨æ–‡")

    success_count = 0
    for article in articles:
        title = article['article_title'] or ''
        link = article['article_link']
        article_id = article['id']
        print(f"  æŠ“å–: {title[:50]}...")
        print(f"  é“¾æ¥: {link}")

        full_text = fetch_full_text(link)

        if full_text:
            c.execute(
                "UPDATE articles SET content = ?, fulltext_fetched = 1 WHERE id = ?",
                (full_text, article_id)
            )
            conn.commit()
            success_count += 1
            print(f"  âœ… æˆåŠŸ: {len(full_text)} å­—ç¬¦")
        else:
            c.execute(
                "UPDATE articles SET fulltext_fetched = 0 WHERE id = ?",
                (article_id,)
            )
            conn.commit()
            print(f"  âŒ å¤±è´¥: æ— æ³•æŠ“å–å…¨æ–‡")

        time.sleep(1)  # ç¤¼è²Œæ€§å»¶è¿Ÿ

    conn.close()
    print(f"âœ… å…¨æ–‡æŠ“å–å®Œæˆ: {success_count}/{len(articles)} æˆåŠŸ")
    return success_count

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="å…¨æ–‡æŠ“å–å·¥å…·")
    parser.add_argument("--limit", type=int, default=20, help="æœ€å¤šå¤„ç†å¤šå°‘ç¯‡")
    parser.add_argument("--force", action="store_true", help="å¼ºåˆ¶é‡æŠ“")
    parser.add_argument("--feed", type=str, default=None, help="ä»…å¤„ç†æŒ‡å®šæºåç§°")
    parser.add_argument("--days", type=int, default=None, help="ä»…å¤„ç†æœ€è¿‘Nå¤©")
    args = parser.parse_args()

    update_articles_with_fulltext(
        limit=args.limit,
        force=args.force,
        feed_name=args.feed,
        days=args.days,
    )
