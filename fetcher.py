#!/usr/bin/env python3
"""
AI RSS æŠ“å–æ¨¡å— - å¢é‡ç‰ˆ
æ¯æ¬¡åªæŠ“å–æœ€æ–°æ–‡ç« ï¼Œé¿å…é‡å¤å’Œæµªè´¹
"""

import feedparser
import requests
from datetime import datetime
import time
import sqlite3
import os

# è¿‡æ»¤éæ³•XMLæ§åˆ¶å­—ç¬¦
def _clean_xml_bytes(data: bytes) -> bytes:
    return bytes(
        b for b in data
        if b in (9, 10, 13) or b >= 32
    )

# ========== æ•°æ®åº“é…ç½® ==========
DB_PATH = os.path.join(os.path.dirname(__file__), 'data', 'ai_rss.db')

def init_db():
    """ç¡®ä¿æ•°æ®åº“å’Œè¡¨å­˜åœ¨"""
    os.makedirs(os.path.dirname(DB_PATH), exist_ok=True)
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    c.execute('''
        CREATE TABLE IF NOT EXISTS articles (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            feed_name TEXT,
            feed_url TEXT,
            feed_priority TEXT DEFAULT 'medium',
            article_title TEXT,
            article_link TEXT UNIQUE,
            published_date TIMESTAMP,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            last_seen TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            content TEXT,
            raw_content TEXT,
            fulltext_fetched INTEGER DEFAULT 0,
            criteria TEXT,
            criteria_score REAL,
            criteria_reason TEXT,
            summary TEXT,
            is_read INTEGER DEFAULT 0
        )
    ''')
    c.execute('CREATE INDEX IF NOT EXISTS idx_article_link ON articles(article_link)')
    c.execute('CREATE INDEX IF NOT EXISTS idx_published_date ON articles(published_date)')
    c.execute('CREATE INDEX IF NOT EXISTS idx_feed_name ON articles(feed_name)')
    c.execute('CREATE INDEX IF NOT EXISTS idx_last_seen ON articles(last_seen)')
    conn.commit()
    conn.close()

def get_latest_published_time(feed_name):
    """è·å–æŸä¸ªæºæœ€æ–°çš„æ–‡ç« å‘å¸ƒæ—¶é—´"""
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    c.execute('''
        SELECT MAX(published_date) FROM articles 
        WHERE feed_name = ? AND published_date IS NOT NULL
    ''', (feed_name,))
    result = c.fetchone()[0]
    conn.close()
    return result

def save_articles_to_db(articles_list, feed_name, feed_url, criteria=""):
    """ä¿å­˜æ–‡ç« åˆ—è¡¨åˆ°æ•°æ®åº“ï¼ˆå¢é‡ï¼‰"""
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    
    saved_count = 0
    now = datetime.now()
    
    for article in articles_list:
        try:
            # å…ˆæ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            c.execute('SELECT id FROM articles WHERE article_link = ?', (article.get('link', ''),))
            existing = c.fetchone()
            
            if existing:
                # å·²å­˜åœ¨ï¼Œæ›´æ–° last_seen
                c.execute('''
                    UPDATE articles 
                    SET last_seen = ? 
                    WHERE id = ?
                ''', (now, existing[0]))
            else:
                # ä¸å­˜åœ¨ï¼Œæ’å…¥æ–°æ–‡ç« 
                c.execute('''
                    INSERT INTO articles 
                    (feed_name, feed_url, article_title, article_link, published_date, raw_content, criteria, last_seen)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    feed_name,
                    feed_url,
                    article.get('title', 'æ— æ ‡é¢˜'),
                    article.get('link', ''),
                    article.get('published', datetime.now()),
                    article.get('summary', '')[:2000],
                    criteria,
                    now
                ))
                saved_count += 1
                
        except Exception as e:
            print(f"    âš ï¸ ä¿å­˜å¤±è´¥: {e}")
    
    conn.commit()
    conn.close()
    return saved_count

# ========== RSSæŠ“å–æ ¸å¿ƒï¼ˆå¢é‡ç‰ˆï¼‰=========

def fetch_articles_from_feed(feed_url, feed_name, max_retries=3, max_entries=30):
    """
    å¢é‡æŠ“å–RSSæº - åªæŠ“å–æœ€æ–°æ–‡ç« 
    
    å‚æ•°:
        feed_url: RSSæºçš„URLåœ°å€
        feed_name: æºçš„åå­—ï¼ˆç”¨äºæ—¥å¿—è¾“å‡ºï¼‰
        max_retries: ç½‘ç»œè¯·æ±‚å¤±è´¥æ—¶çš„æœ€å¤§é‡è¯•æ¬¡æ•°
        max_entries: æ¯æ¬¡æœ€å¤šå¤„ç†å¤šå°‘ç¯‡ï¼ˆé˜²æ­¢æŸäº›æºä¸€æ¬¡æ€§æ¨é€å¤ªå¤šï¼‰
    
    è¿”å›:
        æ–°å¢æ–‡ç« æ•°é‡
    """
    
    # è·å–è¿™ä¸ªæºæœ€æ–°çš„æ–‡ç« æ—¶é—´
    latest_time = get_latest_published_time(feed_name)
    if latest_time:
        print(f"  â”œâ”€ ğŸ“… ä¸Šæ¬¡æœ€æ–°æ–‡ç« : {latest_time}")
    
    new_articles = []
    
    for attempt in range(max_retries):
        try:
            # è®¾ç½®è¯·æ±‚å¤´
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                'Connection': 'keep-alive',
            }
            
            print(f"  â”œâ”€ æŠ“å– {feed_name} (å°è¯• {attempt + 1}/{max_retries})...")
            response = requests.get(feed_url, headers=headers, timeout=30)
            response.raise_for_status()
            
            # è§£æRSSï¼ˆå¿…è¦æ—¶æ¸…æ´—éæ³•å­—ç¬¦ï¼‰
            content = response.content
            feed_data = feedparser.parse(content)
            if feed_data.bozo:
                print(f"  â”œâ”€ è­¦å‘Š: è§£ææœ‰å°é—®é¢˜ï¼Œå°è¯•æ¸…æ´—éæ³•å­—ç¬¦...")
                cleaned = _clean_xml_bytes(content)
                if cleaned != content:
                    feed_data = feedparser.parse(cleaned)
            
            if feed_data.bozo:
                print(f"  â”œâ”€ è­¦å‘Š: è§£æä»æœ‰é—®é¢˜ï¼Œä½†ç»§ç»­...")
            
            total_entries = len(feed_data.entries)
            print(f"  â”œâ”€ RSSåŒ…å« {total_entries} ç¯‡æ–‡ç« ")
            
            # åªå¤„ç†æœ€æ–°çš„ max_entries ç¯‡
            entries_to_process = feed_data.entries[:max_entries]
            
            # æå–æ–‡ç« ä¿¡æ¯ï¼Œåªä¿ç•™æ¯” latest_time æ–°çš„
            new_count = 0
            for entry in entries_to_process:
                # å¤„ç†å‘å¸ƒæ—¶é—´
                published_time = None
                for time_field in ['published_parsed', 'updated_parsed', 'created_parsed']:
                    if hasattr(entry, time_field) and getattr(entry, time_field):
                        published_time = datetime.fromtimestamp(time.mktime(getattr(entry, time_field)))
                        break
                
                if not published_time:
                    published_time = datetime.now()
                
                # å¢é‡åˆ¤æ–­ï¼šå¦‚æœæœ‰ä¸Šæ¬¡æ—¶é—´ï¼Œä¸”è¿™ç¯‡æ–‡ç« æ›´æ—§ï¼Œè·³è¿‡
                if latest_time:
                    # å°†latest_timeå­—ç¬¦ä¸²è½¬æ¢ä¸ºdatetimeå¯¹è±¡è¿›è¡Œæ¯”è¾ƒ
                    try:
                        latest_dt = datetime.strptime(latest_time, '%Y-%m-%d %H:%M:%S')
                        if published_time <= latest_dt:
                            continue
                    except ValueError:
                        # å¦‚æœæ—¶é—´æ ¼å¼ä¸åŒ¹é…ï¼Œè·³è¿‡æ¯”è¾ƒ
                        pass
                
                # æ„å»ºæ–‡ç« å­—å…¸
                article = {
                    'title': entry.get('title', 'æ— æ ‡é¢˜'),
                    'link': entry.get('link', ''),
                    'published': published_time,
                    'summary': entry.get('summary', entry.get('description', ''))[:2000],
                }
                new_articles.append(article)
                new_count += 1
            
            print(f"  â”œâ”€ ğŸ” å‘ç° {new_count} ç¯‡æ–°æ–‡ç« ")
            
            # æŠ“å–æˆåŠŸï¼Œè·³å‡ºé‡è¯•å¾ªç¯
            break
            
        except requests.exceptions.Timeout:
            print(f"  â”œâ”€ è¶…æ—¶")
            if attempt < max_retries - 1:
                time.sleep(3)
        except requests.exceptions.RequestException as e:
            print(f"  â”œâ”€ ç½‘ç»œé”™è¯¯: {e}")
            break
        except Exception as e:
            print(f"  â”œâ”€ è§£æé”™è¯¯: {e}")
            break
    
    # ========== ä¿å­˜åˆ°æ•°æ®åº“ ==========
    if new_articles:
        try:
            # ä»config.pyè·å–criteria
            try:
                from config import RSS_FEEDS
                criteria = ""
                for feed in RSS_FEEDS:
                    if feed['name'] == feed_name:
                        criteria = feed.get('criteria', '')
                        break
            except ImportError:
                criteria = ""
            
            # ä¿å­˜åˆ°æ•°æ®åº“
            saved = save_articles_to_db(new_articles, feed_name, feed_url, criteria)
            print(f"  â”œâ”€ ğŸ’¾ æ–°å¢ {saved} ç¯‡")
            
        except Exception as e:
            print(f"  â”œâ”€ âš ï¸ æ•°æ®åº“ä¿å­˜å¤±è´¥: {e}")
    else:
        print(f"  â”œâ”€ âœ¨ æ²¡æœ‰æ–°æ–‡ç« ")
    
    return len(new_articles)


# ========== å…¨æ–‡æŠ“å–æ¨¡å—ï¼ˆå¢é‡æ„ŸçŸ¥ï¼‰=========

def fetch_full_text_for_recent(limit=50, max_age_days=7):
    """
    ä¸ºæœ€è¿‘æœªæŠ“å–å…¨æ–‡çš„æ–‡ç« è¡¥å…¨æ­£æ–‡
    åªå¤„ç†æœ€è¿‘ max_age_days å¤©å†…çš„æ–‡ç« 
    """
    
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    
    # åªå¤„ç†æœ€è¿‘çš„æ–‡ç« ï¼Œé¿å…æŠ“å–å¤ªæ—§çš„
    cutoff_date = datetime.now().timestamp() - (max_age_days * 24 * 3600)
    
    c.execute('''
        SELECT id, article_link, article_title, feed_name
        FROM articles 
        WHERE (content IS NULL OR content = '' OR fulltext_fetched = 0)
        AND article_link IS NOT NULL
        AND article_link != ''
        AND (published_date IS NULL OR strftime('%s', published_date) > ?)
        ORDER BY 
            CASE WHEN fulltext_fetched = -1 THEN 1 ELSE 0 END,
            published_date DESC 
        LIMIT ?
    ''', (cutoff_date, limit))
    
    articles = c.fetchall()
    
    if not articles:
        print(f"  â”œâ”€ ğŸ“„ æ²¡æœ‰éœ€è¦æŠ“å–å…¨æ–‡çš„æ–‡ç« ")
        conn.close()
        return 0
    
    print(f"  â”œâ”€ ğŸ“„ éœ€è¦æŠ“å–å…¨æ–‡: {len(articles)} ç¯‡")
    
    try:
        from bs4 import BeautifulSoup
        from readability import Document
        import trafilatura
    except ImportError:
        print(f"  â”œâ”€ âš ï¸ ç¼ºå°‘ä¾èµ–åº“")
        conn.close()
        return 0
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
    }
    
    success_count = 0
    for article_id, url, title, feed_name in articles:
        print(f"   æŠ“å–: {feed_name} - {title[:40]}...")
        
        full_text = None
        
        # ç­–ç•¥1: trafilatura
        try:
            downloaded = trafilatura.fetch_url(url, headers=headers)
            if downloaded:
                text = trafilatura.extract(downloaded, include_comments=False)
                if text and len(text) > 200:
                    full_text = text
        except Exception:
            pass
        
        # ç­–ç•¥2: readability
        if not full_text:
            try:
                resp = requests.get(url, headers=headers, timeout=15)
                doc = Document(resp.text)
                soup = BeautifulSoup(doc.summary(), 'html.parser')
                text = soup.get_text()
                if len(text) > 200:
                    full_text = text
            except Exception:
                pass
        
        if full_text:
            c.execute('''
                UPDATE articles 
                SET content = ?, fulltext_fetched = 1 
                WHERE id = ?
            ''', (full_text[:30000], article_id))
            conn.commit()
            success_count += 1
            print(f"      âœ… æˆåŠŸ")
        else:
            c.execute('UPDATE articles SET fulltext_fetched = -1 WHERE id = ?', (article_id,))
            conn.commit()
            print(f"      âŒ å¤±è´¥")
        
        time.sleep(1)
    
    conn.close()
    print(f"  â”œâ”€ âœ… å®Œæˆ: {success_count}/{len(articles)}")
    return success_count


# ========== æ¸…ç†æ—§æ–‡ç« ï¼ˆå¯é€‰ï¼‰=========

def cleanup_old_articles(days=30):
    """åˆ é™¤è¶…è¿‡ days å¤©æœªå‡ºç°çš„æ–‡ç« ï¼ˆå³æºå·²åˆ é™¤çš„æ—§æ–‡ï¼‰"""
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    
    cutoff = datetime.now().timestamp() - (days * 24 * 3600)
    c.execute('''
        DELETE FROM articles 
        WHERE strftime('%s', last_seen) < ?
    ''', (cutoff,))
    
    deleted = c.rowcount
    conn.commit()
    conn.close()
    print(f"ğŸ§¹ æ¸…ç†äº† {deleted} ç¯‡è¶…è¿‡ {days} å¤©æœªå‡ºç°çš„æ—§æ–‡ç« ")
    return deleted


# ========== æ‰¹é‡æŠ“å–æ‰€æœ‰æº ==========

def fetch_all_feeds(max_entries_per_feed=30):
    """æŠ“å–æ‰€æœ‰é…ç½®çš„æºï¼ˆå¢é‡æ¨¡å¼ï¼‰"""
    try:
        from config import RSS_FEEDS
    except ImportError:
        print("âŒ æ‰¾ä¸åˆ° config.py")
        return
    
    def _is_enabled(feed):
        return feed.get("enabled", True) is not False

    print("=" * 60)
    print(f"ğŸš€ å¼€å§‹å¢é‡æŠ“å– {len(RSS_FEEDS)} ä¸ªæº")
    print("=" * 60)
    
    total_new = 0
    for i, feed in enumerate(RSS_FEEDS, 1):
        if not _is_enabled(feed):
            print(f"\n[{i}/{len(RSS_FEEDS)}] {feed['name']} (å·²ç¦ç”¨ï¼Œè·³è¿‡)")
            continue
        print(f"\n[{i}/{len(RSS_FEEDS)}] {feed['name']}")
        new = fetch_articles_from_feed(
            feed['url'], 
            feed['name'], 
            max_entries=max_entries_per_feed
        )
        total_new += new
        time.sleep(1)
    
    print("\n" + "=" * 60)
    print(f"âœ… æŠ“å–å®Œæˆï¼Œå…±æ–°å¢ {total_new} ç¯‡æ–‡ç« ")
    print("=" * 60)
    return total_new


# ========== å‘½ä»¤è¡Œå…¥å£ ==========

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1:
        if sys.argv[1] == "--fulltext":
            print("ğŸ“„ è¿è¡Œå…¨æ–‡æŠ“å–...")
            fetch_full_text_for_recent(limit=50)
        elif sys.argv[1] == "--cleanup":
            days = int(sys.argv[2]) if len(sys.argv) > 2 else 30
            cleanup_old_articles(days)
        elif sys.argv[1] == "--init":
            init_db()
            print("âœ… æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")
        else:
            print("ç”¨æ³•: python fetcher.py [--fulltext|--cleanup [å¤©æ•°]|--init]")
    else:
        # é»˜è®¤ï¼šå¢é‡æŠ“å–æ‰€æœ‰æº
        fetch_all_feeds(max_entries_per_feed=30)
